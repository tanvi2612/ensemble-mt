nohup: ignoring input
Using SCRIPTS_ROOTDIR: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Thu Oct 15 19:50:47 IST 2020
Executing: mkdir -p /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus
(1.0) selecting factors @ Thu Oct 15 19:50:47 IST 2020
(1.1) running mkcls  @ Thu Oct 15 19:50:47 IST 2020
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/mkcls -c50 -n2 -p/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi -V/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb.classes opt
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/mkcls -c50 -n2 -p/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi -V/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 8798

start-costs: MEAN: 1.1962e+06 (1.19583e+06-1.19657e+06)  SIGMA:368.996   
  end-costs: MEAN: 1.09379e+06 (1.09298e+06-1.0946e+06)  SIGMA:806.243   
   start-pp: MEAN: 528.019 (526.167-529.871)  SIGMA:1.85179   
     end-pp: MEAN: 199.498 (197.969-201.026)  SIGMA:1.52868   
 iterations: MEAN: 216217 (208347-224087)  SIGMA:7870   
       time: MEAN: 5.72406 (5.54976-5.89835)  SIGMA:0.174296   
(1.1) running mkcls  @ Thu Oct 15 19:51:02 IST 2020
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/mkcls -c50 -n2 -p/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en -V/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb.classes opt
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/mkcls -c50 -n2 -p/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en -V/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 9367

start-costs: MEAN: 1.13487e+06 (1.13433e+06-1.13542e+06)  SIGMA:546.173   
  end-costs: MEAN: 1.03644e+06 (1.036e+06-1.03689e+06)  SIGMA:442.262   
   start-pp: MEAN: 491.481 (488.818-494.143)  SIGMA:2.66258   
     end-pp: MEAN: 185.135 (184.323-185.947)  SIGMA:0.812149   
 iterations: MEAN: 223284 (218934-227634)  SIGMA:4350   
       time: MEAN: 5.66153 (5.49388-5.82917)  SIGMA:0.167643   
(1.2) creating vcb file /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb @ Thu Oct 15 19:51:18 IST 2020
(1.2) creating vcb file /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb @ Thu Oct 15 19:51:18 IST 2020
(1.3) numberizing corpus /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt @ Thu Oct 15 19:51:18 IST 2020
(1.3) numberizing corpus /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt @ Thu Oct 15 19:51:18 IST 2020
(2) running giza @ Thu Oct 15 19:51:19 IST 2020
(2.1a) running snt2cooc hi-en @ Thu Oct 15 19:51:19 IST 2020

Executing: mkdir -p /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/snt2cooc.out /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/snt2cooc.out /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc
line 1000
line 2000
line 3000
line 4000
END.
(2.1b) running giza hi-en @ Thu Oct 15 19:51:25 IST 2020
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb
Parameter 'coocurrencefile' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.cooc'
Parameter 'c' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2020-10-15.195125.es4' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-10-15.195125.es4.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb  (source vocabulary file name)
t = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-10-15.195125.es4.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb  (source vocabulary file name)
t = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb
Reading vocabulary file from:/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb
Source vocabulary list has 9368 unique tokens 
Target vocabulary list has 8799 unique tokens 
Calculating vocabulary frequencies from corpus /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 4709 sentence pairs.
 Train total # sentence pairs (weighted): 4709
Size of source portion of the training corpus: 96107 tokens
Size of the target portion of the training corpus: 100506 tokens 
In source portion of the training corpus, only 9367 unique tokens appeared
In target portion of the training corpus, only 8797 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 100506/(100816-4709)== 1.04577
There are 871099 871099 entries in table
==========================================================
Model1 Training Started at: Thu Oct 15 19:51:26 2020

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.3018 PERPLEXITY 10097.8
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 17.896 PERPLEXITY 243914
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.95274 PERPLEXITY 123.875
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.42357 PERPLEXITY 686.718
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.37242 PERPLEXITY 82.8493
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.44345 PERPLEXITY 348.122
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 6.0985 PERPLEXITY 68.5223
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.8086 PERPLEXITY 224.194
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.9689 PERPLEXITY 62.635
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.4533 PERPLEXITY 175.253
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 3 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 9367  #classes: 51
Read classes: #words: 8798  #classes: 51

==========================================================
Hmm Training Started at: Thu Oct 15 19:51:29 2020

-----------
Hmm: Iteration 1
A/D table contains 147726 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.85562 PERPLEXITY 57.9052
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.24173 PERPLEXITY 151.348

Hmm Iteration: 1 took: 5 seconds

-----------
Hmm: Iteration 2
A/D table contains 147726 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.72307 PERPLEXITY 52.822
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.50525 PERPLEXITY 90.8394

Hmm Iteration: 2 took: 5 seconds

-----------
Hmm: Iteration 3
A/D table contains 147726 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.39615 PERPLEXITY 42.1118
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.92075 PERPLEXITY 60.5793

Hmm Iteration: 3 took: 5 seconds

-----------
Hmm: Iteration 4
A/D table contains 147726 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 5.14564 PERPLEXITY 35.3992
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.53677 PERPLEXITY 46.423

Hmm Iteration: 4 took: 5 seconds

-----------
Hmm: Iteration 5
A/D table contains 147726 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.97932 PERPLEXITY 31.5445
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.29543 PERPLEXITY 39.272

Hmm Iteration: 5 took: 5 seconds

Entire Hmm Training took: 25 seconds
==========================================================
Read classes: #words: 9367  #classes: 51
Read classes: #words: 8798  #classes: 51
Read classes: #words: 9367  #classes: 51
Read classes: #words: 8798  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Thu Oct 15 19:51:55 2020


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 816.366 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 147726 parameters.
A/D table contains 166307 parameters.
NTable contains 93680 parameter.
p0_count is 76319.6 and p1 is 12093.2; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 5.00358 PERPLEXITY 32.0796
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 5.14966 PERPLEXITY 35.498

THTo3 Viterbi Iteration : 1 took: 4 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 819.218 #alsophisticatedcountcollection: 0 #hcsteps: 4.09195
#peggingImprovements: 0
A/D table contains 147726 parameters.
A/D table contains 165788 parameters.
NTable contains 93680 parameter.
p0_count is 92275.2 and p1 is 4115.39; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.23836 PERPLEXITY 75.4976
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 6.33881 PERPLEXITY 80.9417

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 819.32 #alsophisticatedcountcollection: 0 #hcsteps: 4.1958
#peggingImprovements: 0
A/D table contains 147726 parameters.
A/D table contains 165998 parameters.
NTable contains 93680 parameter.
p0_count is 94557.3 and p1 is 2974.33; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.03619 PERPLEXITY 65.6257
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 6.11886 PERPLEXITY 69.496

Model3 Viterbi Iteration : 3 took: 4 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 819.34 #alsophisticatedcountcollection: 53.8314 #hcsteps: 4.27416
#peggingImprovements: 0
D4 table contains 522319 parameters.
A/D table contains 147726 parameters.
A/D table contains 165921 parameters.
NTable contains 93680 parameter.
p0_count is 95385.3 and p1 is 2560.37; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 5.97977 PERPLEXITY 63.1087
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 6.05662 PERPLEXITY 66.5619

T3To4 Viterbi Iteration : 4 took: 4 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 819.327 #alsophisticatedcountcollection: 50.6475 #hcsteps: 3.80951
#peggingImprovements: 0
D4 table contains 522319 parameters.
A/D table contains 147726 parameters.
A/D table contains 169671 parameters.
NTable contains 93680 parameter.
p0_count is 94278 and p1 is 3114.02; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.04343 PERPLEXITY 65.956
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 6.10588 PERPLEXITY 68.8739

Model4 Viterbi Iteration : 5 took: 14 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 819.213 #alsophisticatedcountcollection: 41.6122 #hcsteps: 3.66745
#peggingImprovements: 0
D4 table contains 522319 parameters.
A/D table contains 147726 parameters.
A/D table contains 169175 parameters.
NTable contains 93680 parameter.
p0_count is 94056.9 and p1 is 3224.56; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.86232 PERPLEXITY 58.1747
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 5.9123 PERPLEXITY 60.2256

Model4 Viterbi Iteration : 6 took: 15 seconds
H333444 Training Finished at: Thu Oct 15 19:52:39 2020


Entire Viterbi H333444 Training took: 44 seconds
==========================================================

Entire Training took: 74 seconds
Program Finished at: Thu Oct 15 19:52:39 2020

==========================================================
Executing: rm -f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.A3.final.gz
Executing: gzip /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.A3.final
(2.1a) running snt2cooc en-hi @ Thu Oct 15 19:52:39 IST 2020

Executing: mkdir -p /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/snt2cooc.out /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/snt2cooc.out /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc
line 1000
line 2000
line 3000
line 4000
END.
(2.1b) running giza en-hi @ Thu Oct 15 19:52:46 IST 2020
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb
Executing: /home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb
/home/es4/sem5/nlp/ensemble-mt/moses/moses/tools/GIZA++  -CoocurrenceFile /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi -onlyaldumps 1 -p0 0.999 -s /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb -t /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.cooc'
Parameter 'c' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2020-10-15.195246.es4' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb'
Parameter 't' changed from '' to '/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-10-15.195246.es4.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb  (source vocabulary file name)
t = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2020-10-15.195246.es4.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb  (source vocabulary file name)
t = /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/hi.vcb
Reading vocabulary file from:/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en.vcb
Source vocabulary list has 8799 unique tokens 
Target vocabulary list has 9368 unique tokens 
Calculating vocabulary frequencies from corpus /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/corpus/en-hi-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 4709 sentence pairs.
 Train total # sentence pairs (weighted): 4709
Size of source portion of the training corpus: 100506 tokens
Size of the target portion of the training corpus: 96107 tokens 
In source portion of the training corpus, only 8798 unique tokens appeared
In target portion of the training corpus, only 9366 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 96107/(105215-4709)== 0.956231
There are 871668 871668 entries in table
==========================================================
Model1 Training Started at: Thu Oct 15 19:52:47 2020

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 13.4104 PERPLEXITY 10887.6
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 18.0478 PERPLEXITY 270974
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.95855 PERPLEXITY 124.375
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.48211 PERPLEXITY 715.152
Model 1 Iteration: 2 took: 1 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 6.33946 PERPLEXITY 80.9779
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.42939 PERPLEXITY 344.746
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 6.06169 PERPLEXITY 66.7962
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.79151 PERPLEXITY 221.553
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.93485 PERPLEXITY 61.174
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.44204 PERPLEXITY 173.891
Model 1 Iteration: 5 took: 1 seconds
Entire Model1 Training took: 2 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 8798  #classes: 51
Read classes: #words: 9367  #classes: 51

==========================================================
Hmm Training Started at: Thu Oct 15 19:52:49 2020

-----------
Hmm: Iteration 1
A/D table contains 172284 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.84378 PERPLEXITY 57.4318
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.236 PERPLEXITY 150.748

Hmm Iteration: 1 took: 6 seconds

-----------
Hmm: Iteration 2
A/D table contains 172284 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.73076 PERPLEXITY 53.1046
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.49874 PERPLEXITY 90.4307

Hmm Iteration: 2 took: 5 seconds

-----------
Hmm: Iteration 3
A/D table contains 172284 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 5.38365 PERPLEXITY 41.7485
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.8895 PERPLEXITY 59.2809

Hmm Iteration: 3 took: 6 seconds

-----------
Hmm: Iteration 4
A/D table contains 172284 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 5.13342 PERPLEXITY 35.1005
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.50907 PERPLEXITY 45.5403

Hmm Iteration: 4 took: 6 seconds

-----------
Hmm: Iteration 5
A/D table contains 172284 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.96844 PERPLEXITY 31.3075
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.26797 PERPLEXITY 38.5316

Hmm Iteration: 5 took: 6 seconds

Entire Hmm Training took: 29 seconds
==========================================================
Read classes: #words: 8798  #classes: 51
Read classes: #words: 9367  #classes: 51
Read classes: #words: 8798  #classes: 51
Read classes: #words: 9367  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Thu Oct 15 19:53:18 2020


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 786.933 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 172284 parameters.
A/D table contains 144065 parameters.
NTable contains 87990 parameter.
p0_count is 70368.8 and p1 is 12869.1; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 4.91953 PERPLEXITY 30.2641
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 5.05624 PERPLEXITY 33.2721

THTo3 Viterbi Iteration : 1 took: 5 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 790.647 #alsophisticatedcountcollection: 0 #hcsteps: 4.08431
#peggingImprovements: 0
A/D table contains 172284 parameters.
A/D table contains 143866 parameters.
NTable contains 87990 parameter.
p0_count is 86782.2 and p1 is 4662.42; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.26049 PERPLEXITY 76.6649
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 6.37012 PERPLEXITY 82.7176

Model3 Viterbi Iteration : 2 took: 3 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 790.788 #alsophisticatedcountcollection: 0 #hcsteps: 4.30176
#peggingImprovements: 0
A/D table contains 172284 parameters.
A/D table contains 143725 parameters.
NTable contains 87990 parameter.
p0_count is 90047.9 and p1 is 3029.53; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 6.04304 PERPLEXITY 65.9383
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 6.13579 PERPLEXITY 70.3165

Model3 Viterbi Iteration : 3 took: 3 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 790.782 #alsophisticatedcountcollection: 57.4746 #hcsteps: 4.40964
#peggingImprovements: 0
D4 table contains 518868 parameters.
A/D table contains 172284 parameters.
A/D table contains 143725 parameters.
NTable contains 87990 parameter.
p0_count is 91057 and p1 is 2525.02; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 5.9867 PERPLEXITY 63.4127
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 6.07331 PERPLEXITY 67.3364

T3To4 Viterbi Iteration : 4 took: 5 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 790.799 #alsophisticatedcountcollection: 57.367 #hcsteps: 3.95137
#peggingImprovements: 0
D4 table contains 518868 parameters.
A/D table contains 172284 parameters.
A/D table contains 145178 parameters.
NTable contains 87990 parameter.
p0_count is 89837.8 and p1 is 3134.59; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 6.12185 PERPLEXITY 69.6402
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 6.19565 PERPLEXITY 73.2951

Model4 Viterbi Iteration : 5 took: 13 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 790.761 #alsophisticatedcountcollection: 48.2058 #hcsteps: 3.81928
#peggingImprovements: 0
D4 table contains 518868 parameters.
A/D table contains 172284 parameters.
A/D table contains 145005 parameters.
NTable contains 87990 parameter.
p0_count is 89570.5 and p1 is 3268.27; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 5.95824 PERPLEXITY 62.1739
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 6.01935 PERPLEXITY 64.8641

Model4 Viterbi Iteration : 6 took: 14 seconds
H333444 Training Finished at: Thu Oct 15 19:54:01 2020


Entire Viterbi H333444 Training took: 43 seconds
==========================================================

Entire Training took: 75 seconds
Program Finished at: Thu Oct 15 19:54:01 2020

==========================================================
Executing: rm -f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.A3.final.gz
Executing: gzip /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.A3.final
(3) generate word alignment @ Thu Oct 15 19:54:01 IST 2020
Combining forward and inverted alignment from files:
  /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.A3.final.{bz2,gz}
  /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.A3.final.{bz2,gz}
Executing: mkdir -p /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/training/giza2bal.pl -d "gzip -cd /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.en-hi/en-hi.A3.final.gz" -i "gzip -cd /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/giza.hi-en/hi-en.A3.final.gz" |/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<4709>
(4) generate lexical translation table 0-0 @ Thu Oct 15 19:54:04 IST 2020
(/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi,/home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en,/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex)
!!!!!
Saved: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.f2e and /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.e2f
FILE: /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en
FILE: /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi
FILE: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Thu Oct 15 19:54:05 IST 2020
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/extract-parallel.perl 12 split "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/extract /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/aligned.grow-diag-final-and /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/extract-parallel.perl 12 split "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/extract /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/aligned.grow-diag-final-and /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Thu Oct 15 19:54:05 2020
using gzip 
isBSDSplit=0 
Executing: mkdir -p /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900; ls -l /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900 
total=4709 line-per-split=393 
split -d -l 393 -a 7 /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.hi /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/source.split -d -l 393 -a 7 /home/es4/sem5/nlp/ensemble-mt/dataset/MKB/train.tok.clean.en /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/target.split -d -l 393 -a 7 /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/aligned.grow-diag-final-and /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/align.merging extract / extract.inv
gunzip -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000000.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000001.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000002.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000003.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000004.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000005.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000006.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000007.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000008.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000009.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000010.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000011.gz  | LC_ALL=C sort     -T /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900 2>> /dev/stderr | gzip -c > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000000.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000001.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000002.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000003.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000004.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000005.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000006.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000007.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000008.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000009.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000010.inv.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000011.inv.gz  | LC_ALL=C sort     -T /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900 2>> /dev/stderr | gzip -c > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000000.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000001.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000002.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000003.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000004.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000005.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000006.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000007.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000008.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000009.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000010.o.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900/extract.0000011.o.gz  | LC_ALL=C sort     -T /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.19900 2>> /dev/stderr | gzip -c > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Thu Oct 15 19:54:52 2020
(6) score phrases @ Thu Oct 15 19:54:52 IST 2020
(6.1)  creating table half /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.f2e @ Thu Oct 15 19:54:52 IST 2020
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/score-parallel.perl 12 "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.sorted.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.f2e /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/score-parallel.perl 12 "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.sorted.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.f2e /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Thu Oct 15 19:54:52 2020
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/extract.0.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.f2e /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/phrase-table.half.0000000.gz  2>> /dev/stderr 
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.0.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.1.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.2.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.3.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.4.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.5.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.6.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.7.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.8.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.9.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.10.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/run.11.shmv /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989/phrase-table.half.0000000.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.f2e.gzrm -rf /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.20989 
Finished Thu Oct 15 19:55:04 2020
(6.3)  creating table half /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.e2f @ Thu Oct 15 19:55:04 IST 2020
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/score-parallel.perl 12 "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.inv.sorted.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.e2f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/generic/score-parallel.perl 12 "sort    " /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.inv.sorted.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.e2f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Thu Oct 15 19:55:04 2020
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/extract.0.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/lex.e2f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.0.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.1.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.2.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.3.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.6.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.4.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.5.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.7.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.8.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.9.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.10.sh/mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/run.11.shgunzip -c /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186  | gzip -c > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/tmp.21186 
Finished Thu Oct 15 19:55:49 2020
(6.6) consolidating the two halves @ Thu Oct 15 19:55:49 IST 2020
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/consolidate /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.f2e.gz /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables
...
Executing: rm -f /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/phrase-table.half.*
(7) learn reordering model @ Thu Oct 15 19:55:54 IST 2020
(7.1) [no factors] learn reordering model @ Thu Oct 15 19:55:54 IST 2020
(7.2) building tables @ Thu Oct 15 19:55:54 IST 2020
Executing: /mnt/c/Users/chait/sem5/nlp/ensemble-mt/moses/moses/scripts/../bin/lexical-reordering-score /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/extract.o.sorted.gz 0.5 /mnt/c/Users/chait/sem5/nlp/ensemble-mt/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Thu Oct 15 19:55:58 IST 2020
  no generation model requested, skipping step
(9) create moses.ini @ Thu Oct 15 19:55:58 IST 2020
